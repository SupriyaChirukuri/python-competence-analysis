# python-competence-analysis
Research plan and reasoning for evaluating open-source AI models (e.g., StarCoder2, CodeBERT) for student competence analysis in Python learning. Includes a 2-paragraph research plan (research_plan.md) and detailed reasoning (README.md) on model suitability, testing strategies, trade-offs, and educational applications.

**Q — What makes a model suitable for high-level competence analysis?**
A model needs to “read for meaning,” not just token patterns. Practically that means: (1) semantic understanding of code (can relate tokens, control flow, and data structures to intended behavior), (2) a long enough context window to consider the whole submission and any comments, (3) instruction-following or controllability so it can be prompted to ask questions instead of producing a patch, (4) predictable behavior (limited hallucination) so educators can trust outputs, and (5) a friendly license and feasible inference cost. Think of the model as an experienced tutor — it should be able to scan a student’s solution, relate it to expected concepts, and ask the right Socratic question.

**Q — How would you test whether a model generates meaningful prompts?**
Use a two-stage test: (1) Automated relevance checks — measure whether generated prompts align with annotated conceptual gaps using embedding similarity and whether the model flags the correct suspicious lines (precision/recall). Add a “non-spoiler” filter that detects whether an output contains direct fixes (keyword patterns like explicit code patches). (2) Human evaluation — give instructors blind samples of model prompts and ask them to rate usefulness, clarity, and whether the prompt is likely to nudge a student to reason deeper (and not just reveal the answer). If possible, run a small student-facing pilot: present model prompts to learners and measure whether their next revision shows improved reasoning (a lightweight A/B test).

**Q — What trade-offs might exist between accuracy, interpretability, and cost?**
There’s no free lunch. Big generative LLMs (higher accuracy/generative quality) cost more to run (GPU time, latency) and are harder to interpret. Smaller encoder models (like CodeBERT embeddings) are cheaper and more transparent for classification tasks but can’t craft nuanced prompts. Interpretable, rule-based checks are cheap and explainable but miss subtle conceptual gaps. So you pick based on your constraint: if you need top-tier generative prompts, accept higher cost and add guardrails; if you need scalable, explainable triage, combine embeddings + rules.

**Q — Why pick the model you evaluated, and what are its strengths or limitations?**
A practical candidate is StarCoder2 (or similar code-specialized open LLMs) because it’s trained heavily on code, provides reasonably long contexts, and is available in multiple sizes so you can trade off accuracy vs. cost. Strengths: good at reading code and producing human-readable prompts, flexible for few-shot instruction, and supported by a community/tooling. Limitations: can hallucinate plausible but incorrect diagnostics, larger variants need significant compute, and out-of-the-box behavior may need prompt engineering or light instruction-tuning to reliably avoid giving full fixes. As a complement, CodeBERT (an encoder) is useful for interpretable embedding-based tasks (e.g., finding similar buggy patterns or clustering common misconceptions) but isn’t generative—so the two together form a pragmatic stack: CodeBERT for detection/interpretability, StarCoder2 for crafting the human-facing prompt.
