I will start by surveying open-source models that are explicitly tuned or proven useful for code understanding and instruction-following—examples to consider are StarCoder2, CodeLlama (code variants), and encoder models like CodeBERT. The selection criteria will prioritize (a) license permissiveness, (b) code-specialization and training data focus (Python examples, docstrings, and tests), (c) context window size (so the model can see full student submissions and surrounding comments), and (d) tooling for running local inference or easy hosted evaluation. I’ll build a small, curated dataset of 50–200 student Python submissions representing common learning scenarios (syntax mistakes, off-by-one/indexing bugs, wrong data types, algorithmic misunderstandings, and missing edge-case handling) and annotate each example with the error type and a short “desired diagnostic prompt” that probes the student’s conceptual gap without giving the fix.

To validate model suitability I’ll run both automated and human-in-the-loop tests. Automated checks will measure whether the model can (a) flag suspicious lines (precision/recall against annotated ground truth), and (b) produce prompts semantically similar to the “desired diagnostic prompts” (embedding similarity, BLEU/ROUGE-style checks for short outputs). Human evaluation will ask 3–5 instructors to rate model-generated prompts on usefulness, clarity, and “non-spoiler” quality (i.e., does the prompt encourage thinking rather than reveal the solution?). I’ll compare zero-shot, few-shot, and lightly fine-tuned variants to see what trade-offs each approach creates in terms of quality, cost, and reliability; finally I’ll document compute/latency, hallucination examples, and practical deployment notes (e.g., whether inference must be cloud-based or can run on smaller hardware).
